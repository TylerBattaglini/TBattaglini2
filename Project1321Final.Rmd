---
title: "Factors Influence Golf Earnings"
author: 'Tyler Battaglini'
date: "2024-09-19"
output:
  word_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    keep_md: yes
  html_document: 
    toc: yes
    toc_depth: 4
    toc_float: yes
    fig_width: 4
    fig_caption: yes
    number_sections: yes
    toc_collapsed: yes
    code_folding: hide
    code_download: yes
    smooth_scroll: yes
    theme: lumen
  pdf_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
    fig_width: 3
    fig_height: 3
editor_options: 
  chunk_output_type: inline
always_allow_html: true
---

```{=html}

<style type="text/css">

/* Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML. it is a simple mechanism for adding style (e.g., fonts, colors, spacing) to Web documents. */

h1.title {  /* Title - font specifications of the report title */
  font-size: 24px;
  font-weight: bold;
  color: DarkRed;
  text-align: center;
  font-family: "Gill Sans", sans-serif;
}
h4.author { /* Header 4 - font specifications for authors  */
  font-size: 20px;
  font-weight: bold;
  font-family: system-ui;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - font specifications for the date  */
  font-size: 18px;
  font-weight: bold;
  font-family: system-ui;
  color: DarkBlue;
  text-align: center;
}
h1 { /* Header 1 - font specifications for level 1 section title  */
    font-size: 22px;
    font-weight: bold;
    font-family: system-ui;
    color: navy;
    text-align: left;
}
h2 { /* Header 2 - font specifications for level 2 section title */
    font-size: 20px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - font specifications of level 3 section title  */
    font-size: 18px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - font specifications of level 4 section title  */
    font-size: 18px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

body { background-color:white; }

.highlightme { background-color:yellow; }

p { background-color:white; }

</style>
```

```{r setup, include=FALSE}
# Detect, install and load packages if needed.
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("MASS")) {
   install.packages("MASS")
   library(MASS)
}
if (!require("nleqslv")) {
   install.packages("nleqslv")
   library(nleqslv)
}
#
if (!require("pander")) {
   install.packages("pander")
   library(pander)
}

if (!require("psych")) {   
  install.packages("psych")
   library(psych)
}
if (!require("MASS")) {   
  install.packages("MASS")
   library(MASS)
}
if (!require("ggplot2")) {   
  install.packages("ggplot2")
   library(ggplot2)
}
if (!require("GGally")) {   
  install.packages("GGally")
   library(GGally)
}
if (!require("car")) {   
  install.packages("car")
   library(car)
}
if (!require("dplyr")) {   
  install.packages("dplyr")
   library(dplyr)
}
if (!require("caret")) {   
  install.packages("caret")
   library(caret)
}

# specifications of outputs of code in code chunks
knitr::opts_chunk$set(echo = TRUE,      # include code chunk in the output file
                      warnings = FALSE,  # sometimes, you code may produce warning messages,
                                         # you can choose to include the warning messages in
                                         # the output file. 
                      messages = FALSE,  #
                      results = TRUE,
                      
                      comment = NA       # you can also decide whether to include the output
                                         # in the output file.
                      )   
```


# Introduction

## Description
The PGA 2004 dataset contains performance statistics and winnings data for 196 participants in the PGA (Proffesional Golfers Association) during the 2004 season. The dataset provides information on several key aspects of player performance and earnings such as:

Name : The name of each golfer.
Age : The age of the player.
Average Drive : The average driving distance of the player in yards
Driving Accuracy: The percentage of drives that land on the fairway, 
Greens in Regulation : The percentage of greens reached in regulation, meaning the number of strokes used by the player to land the ball on the green is two or more less than par.
Average Number of Putts : The average number of putts per round
Save Percentage : The percentage of times a player saves par or better from around the green 
Money Rank : The rank of the player based on total earnings for the season.
Number of Events : The total number of events the player participated in during the season.
Total Winnings : The total amount of money earned by the player over the course of the season.
Average Winnings : The average amount of money earned per event by the player.

### Research Question
We want to investigate what variables affect the players winnings of this given season. We will look at mainly the average drive and how it correlates to winnings.

## Data Preperation

```{r}

url <- "https://users.stat.ufl.edu/~winner/data/pga2004.dat"
pga_data <- read.table(url, header = FALSE, fill = TRUE)
pga_data$V1 <- NULL
colnames(pga_data) <- c("Player_name", "Player_Age", "Average_Drive", "Driving_Accuracy", "Greens_on_reg", 
                        "Average_number_putts", "Save_Percent", "Money_Rank",
                        "Number_events", "Total_Winnings", "Average_Winnings")
head(pga_data)
```

We take a broad look at the data and see there was two categorical variables one of which having the first name of a player and then a second one with the last name. We void the first name and just look at the last names.

```{r}
pander(head(pga_data))

```
```{r}
cor(pga_data[, c("Average_Drive" , "Driving_Accuracy" , "Greens_on_reg" , "Average_number_putts" , "Save_Percent" , "Money_Rank" , "Number_events" , "Total_Winnings" , "Average_Winnings")], use = "pairwise.complete.obs")


```
We see that a lot of our variables have high correlation which can cause multicollinearity. We must deal with this issue before doing our analysis. 

```{r}
colSums(is.na(pga_data))
pga_data_clean <- na.omit(pga_data)
```

For the sake of missing observations we will be removing it for our data analysis because we have a plentiful amount of observations.

```{r}
ggplot(pga_data_clean, aes(x = Average_Drive, y = Average_Winnings)) +
  geom_point() +
  geom_smooth(method = "lm", col = "green") +
  labs(title = "Scatter Plot of Average Drive vs. Average Winnings", x = "Average Drive", y = "Average Winnings")
str(pga_data_clean)
```


We look at an initial graph of average winnigs vs average drive. It looks like we have a good cluster of data but we have a couple of outliers in our data that is scewing our line.
```{r}
pga_data_clean$Age_Above_30 <- pga_data_clean$Player_Age > 30

head(pga_data_clean$Age_Above_30)
pga_data_clean <- pga_data_clean %>% select(-Player_name, -Driving_Accuracy, -Total_Winnings, -Average_number_putts, -Money_Rank, -Player_Age)
kable(head(pga_data_clean))


```

We remove player name and money rank because we are "ranking" them or sorting them by +/- 30 age. We remove driving accuracy because we have a variable with average drive because that is what we are making assumptions on and also it could cause multicollinearity because driving accuracy and average drive are highly correlated. The same can be said for total winnings versus average winnings and also save percent and number of putts. They are both correlated and can cause multicollinearity.

# Model Bulding

## Linear Model

```{r}
colnames(pga_data_clean)
full.model = lm(Average_Winnings ~  Average_Drive + Greens_on_reg +  Save_Percent + Number_events + Age_Above_30, data = pga_data_clean)
kable(summary(full.model)$coef, caption ="Statistics of Regression Coefficients")


```
We transform the age category into a true or false of payers with above or below age 30. All of our values are significant besides Age.
```{r}
par(mfrow=c(2,2))
plot(full.model)


```

It looks like we have faults in our assumptions. Based on the QQ plot there seems to be a non-normal distribution with several outliers. There also seems to be non-constant variance.

```{r}
vif(full.model)
barplot(vif(full.model), main = "VIF Values", horiz = FALSE, col = "steelblue")
```

We see in the vif values above that all of them are below 5 which shows little to no multicollineairty so we can continue.

## Box Cox Transformation

We perform this transformation because our data has non constant variance

```{r}
par(pty = "s", mfrow = c(2, 2), oma=c(.1,.1,.1,.1), mar=c(4, 0, 2, 0))
##
boxcox(Average_Winnings ~ log(Average_Drive) + Greens_on_reg +  Save_Percent + Number_events + Age_Above_30, data = pga_data_clean, lambda = seq(-1, 1, length = 10), 
       xlab=expression(paste(lambda, ": log Average Drive")))
##
boxcox(Average_Winnings ~ Average_Drive + Greens_on_reg +  Save_Percent + Number_events + Age_Above_30, data = pga_data_clean, lambda = seq(-1, 1, length = 10), 
       xlab=expression(paste(lambda, ": Average Drive")))
##
boxcox(Average_Winnings ~ Average_Drive + Greens_on_reg +  Save_Percent + Number_events + log(1+Age_Above_30), data = pga_data_clean, lambda = seq(-1, 1, length = 10), 
       xlab=expression(paste(lambda, ": log-age")))
##
boxcox(Average_Winnings ~ log(Average_Drive) + Greens_on_reg +  Save_Percent + Number_events + log(1+Age_Above_30), data = pga_data_clean, lambda = seq(-1, 1, length = 10), 
      xlab=expression(paste(lambda, ": log-age, log Average Drive")))

```

## Square Root Model

```{r}
sqrt.winnings.log.drive = lm((Average_Winnings)^0.5 ~ Greens_on_reg +  Save_Percent + Number_events + Age_Above_30 + log(Average_Drive), data = pga_data_clean)
kable(summary(sqrt.winnings.log.drive)$coef, caption = "log-transformed model")


```

```{r}
par(mfrow = c(2,2))
plot(sqrt.winnings.log.drive)

```

This model worsened our QQ-plot normality and our non constant variance remained the same, so we can try another model to see if it helps.

## Log Transformation

```{r}
log.winnings = lm(log(Average_Winnings) ~ Average_Drive + Greens_on_reg +  Save_Percent + Number_events + Age_Above_30, data = pga_data_clean)
kable(summary(log.winnings)$coef, caption = "log-transformed model")


```

```{r}
par(mfrow = c(2,2))
plot(log.winnings)

```

This model looks drastically better then the other two models. While normality still cannot be assumed the QQ Plot looks better. The same thing has happened with our Residual plot while constant variance can still not be assumed it is drastically better.

## Comparison of models

```{r}

par(pty = "s", mfrow = c(1, 3))

qqnorm(full.model$residuals, main = "Full-Model")
qqline(full.model$residuals)

qqnorm(log.winnings$residuals, main = "Log Winnings")
qqline(log.winnings$residuals)

qqnorm(sqrt.winnings.log.drive$residuals, main = "sqrt winnings log drive")
qqline(sqrt.winnings.log.drive$residuals)


```

We can see that Log Winnings is the best model for our data.

## Goodness of Fit Comparison

```{r}
select=function(m){ 
 e = m$resid                           
 n0 = length(e)                        
 SSE=(m$df)*(summary(m)$sigma)^2       
 R.sq=summary(m)$r.squared             
 R.adj=summary(m)$adj.r               
 MSE=(summary(m)$sigma)^2              
 Cp=(SSE/MSE)-(n0-2*(n0-m$df))         
 AIC=n0*log(SSE)-n0*log(n0)+2*(n0-m$df)          
 SBC=n0*log(SSE)-n0*log(n0)+(log(n0))*(n0-m$df)  
 X=model.matrix(m)                     
 H=X%*%solve(t(X)%*%X)%*%t(X)          
 d=e/(1-diag(H))                       
 PRESS=t(d)%*%d   
 tbl = as.data.frame(cbind(SSE=SSE, R.sq=R.sq, R.adj = R.adj, Cp = Cp, AIC = AIC, SBC = SBC, PRD = PRESS))
 names(tbl)=c("SSE", "R.sq", "R.adj", "Cp", "AIC", "SBC", "PRESS")
 tbl
 }

```

```{r}

output.sum = rbind(select(full.model), select(sqrt.winnings.log.drive), select(log.winnings))
row.names(output.sum) = c("full.model", "sqrt.winnings.log.drive", "log.winnings")
kable(output.sum, caption = "Goodness-of-fit Measures of Candidate Models")

```

Even though the R, R squared, and Cp look better in the other two models our value are still relatively good for log.winnings. Log.winnings has better residuals, QQ plot, but less goodness of fit measures but they are not significantly bad so we will stick with the log model.

## Final Model

```{r}
kable(summary(log.winnings)$coef, caption = "Inferential Statistics of Final Model")
```

While we do have a large sample of 196 and all of our pvalues are close to 0 we do have one variable "Age" which is not significant.

## Summary of Model
The final model can be represented as log(price)=3.9245 − 0.0246×Average_Drive + 0.1866×Greens_on_reg + 
0.0387×Save_percent − 0.0181×Number_events + 0.1826×Age_Above_30TRUE

From the above data we can also conclude that as average winnings go up our average drive goes down by -2.5% while other variables such as greens on regulation and save percent go up.

```{r}
log.winnings = lm(log(Average_Winnings) ~ Average_Drive + Greens_on_reg + Save_Percent + Number_events + Age_Above_30, data = pga_data_clean)

B = 1000  #

num.p = length(coef(log.winnings))  # number of parameters in the model (includes intercept)
smpl.n = nrow(pga_data_clean)       # sample size

# Matrix to store bootstrap coefficients
coef.mtrx = matrix(0, nrow = B, ncol = num.p)

# Bootstrap loop
for (i in 1:B) {
  bootc.id = sample(1:smpl.n, smpl.n, replace = TRUE)  # Resample with replacement
  boot_data = pga_data_clean[bootc.id, ]  # Create the bootstrap sample
  log.winnings.btc = lm(log(Average_Winnings) ~ Average_Drive + Greens_on_reg + Save_Percent + Number_events + Age_Above_30, data = boot_data)
  coef.mtrx[i, ] = coef(log.winnings.btc)  # Store the coefficients
}

```

# Bootstrapping Model

```{r}

boot.hist = function(cmtrx, bt.coef.mtrx, var.id, var.nm){

  x1.1 <- seq(min(bt.coef.mtrx[,var.id]), max(bt.coef.mtrx[,var.id]), length=300 )
  y1.1 <- dnorm(x1.1, mean(bt.coef.mtrx[,var.id]), sd(bt.coef.mtrx[,var.id]))

  highestbar = max(hist(bt.coef.mtrx[,var.id], plot = FALSE)$density) 
  ylimit <- max(c(y1.1,highestbar))
  hist(bt.coef.mtrx[,var.id], probability = TRUE, main = var.nm, xlab="", 
       col = "azure1",ylim=c(0,ylimit), border="lightseagreen")
  lines(x = x1.1, y = y1.1, col = "red3")
  lines(density(bt.coef.mtrx[,var.id], adjust=2), col="blue") 
  
}

```

```{r}
par(mfrow=c(2,3))  
boot.hist(bt.coef.mtrx=coef.mtrx, var.id=1, var.nm ="Intercept" )
boot.hist(bt.coef.mtrx=coef.mtrx, var.id=2, var.nm ="Average Drive" )
boot.hist(bt.coef.mtrx=coef.mtrx, var.id=3, var.nm ="Greens On Regulation" )
boot.hist(bt.coef.mtrx=coef.mtrx, var.id=4, var.nm ="Save Percentage" )
boot.hist(bt.coef.mtrx=coef.mtrx, var.id=5, var.nm ="Number of Events" )
boot.hist(bt.coef.mtrx=coef.mtrx, var.id=6, var.nm ="Age Above 30" )
```

We see from our histograms that they look to have a equal distribution. The blue curve represents the p-values reported. While the blue curve represents the bootstrap intervals. They are very similar.

## 95% CI

```{r}

num.p = dim(coef.mtrx)[2]  

btc.ci = NULL
btc.wd = NULL

for (i in 1:num.p) {
  lci.025 = round(quantile(coef.mtrx[, i], 0.025, type = 2), 8)
  uci.975 = round(quantile(coef.mtrx[, i], 0.975, type = 2), 8)
  btc.wd[i] = uci.975 - lci.025
  btc.ci[i] = paste("[", round(lci.025, 4), ", ", round(uci.975, 4), "]")
}

mean.coefs = apply(coef.mtrx, 2, mean)

results = as.data.frame(cbind(Mean_Coef = formatC(mean.coefs, 4, format = "f"), btc.ci.95 = btc.ci))

kable(results, caption = "Regression Coefficient Matrix with Bootstrap Confidence Intervals")

```

We can see that our values are consistent with the p-values that we got. Our 95% CI spans over 0 in the Age variable which matches up with our p-value not being significant

```{r}
hist(sort(log.winnings$residuals),n=40,
     xlab="Residuals",
     col = "lightblue",
     border="navy",
     main = "Histogram of Residuals")


```

We appear to have a normal distribution but we do appear to have some outliers on either side of the curve.

## Residual Bootstrap Regression

```{r}
log.winnings <- lm(log(Average_Winnings) ~ Average_Drive + Greens_on_reg + Save_Percent + Number_events + Age_Above_30, data = pga_data_clean)
model.resid = log.winnings$residuals
B=1000
num.p = dim(model.matrix(log.winnings))[2]   
samp.n = dim(model.matrix(log.winnings))[1]  
btr.mtrx = matrix(rep(0,6*B), ncol=num.p)
for (i in 1:B){

  bt.lg.winnings = log.winnings$fitted.values + 
        sample(log.winnings$residuals, samp.n, replace = TRUE)  
  
  pga_data_clean$bt.lg.winnings =  bt.lg.winnings   #  send the boot response to the data
  btr.model = lm(bt.lg.winnings ~ Average_Drive + Greens_on_reg + Save_Percent + Number_events + Age_Above_30, data = pga_data_clean)  
  btr.mtrx[i,]=btr.model$coefficients
}
```

```{r}
boot.hist = function(bt.coef.mtrx, var.id, var.nm){

  x1.1 <- seq(min(bt.coef.mtrx[,var.id]), max(bt.coef.mtrx[,var.id]), length=300 )
  y1.1 <- dnorm(x1.1, mean(bt.coef.mtrx[,var.id]), sd(bt.coef.mtrx[,var.id]))

  highestbar = max(hist(bt.coef.mtrx[,var.id], plot = FALSE)$density) 
  ylimit <- max(c(y1.1,highestbar))
  hist(bt.coef.mtrx[,var.id], probability = TRUE, main = var.nm, xlab="", 
       col = "azure1",ylim=c(0,ylimit), border="lightseagreen")
  lines(x = x1.1, y = y1.1, col = "red3")             
  lines(density(bt.coef.mtrx[,var.id], adjust=2), col="blue")    
} 



```

```{r}
par(mfrow=c(2,3))  
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=1, var.nm ="Intercept" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=2, var.nm ="Average Drive" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=3, var.nm ="Greens on Regulation" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=4, var.nm ="Save Percentage" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=5, var.nm ="Number of Events" )
boot.hist(bt.coef.mtrx=btr.mtrx, var.id=6, var.nm ="Age" )


```

We again see that our histograms have an equal distribution. Again we see that the p-values and residual boostrap have very similar values.
```{r}
um.p = dim(btr.mtrx)[2]  
btr.ci = NULL
btr.wd = NULL

for (i in 1:num.p) {
  lci.025 = round(quantile(btr.mtrx[, i], 0.025, type = 2), 8)
  uci.975 = round(quantile(btr.mtrx[, i], 0.975, type = 2), 8)
  btr.wd[i] = uci.975 - lci.025
  btr.ci[i] = paste("[", round(lci.025, 4), ", ", round(uci.975, 4), "]")
}

kable(as.data.frame(cbind(formatC(btr.mtrx[1, ], 4, format = "f"), btr.ci.95 = btr.ci)), 
      caption = "Regression Coefficient Matrix with 95% Residual Bootstrap CI")

```

Once again we get the same output as expected. The values above show the same results as our p-values, all being significant except for age which includes 0.

# Combining results

```{r}
p_values <- summary(log.winnings)$coefficients[-3, 4]  

combined_matrix <- cbind(
  Coefficients = formatC(coef(log.winnings)[-3], 4, format = "f"),  
  `95% CI (Bootstrap t)` = btc.ci,                                
  `95% CI (Bootstrap r)` = btr.ci,                                 
  `p-values` = formatC(p_values, 4, format = "f")                  
)


library(knitr)
kable(as.data.frame(combined_matrix), 
      caption = "Final Combined Inferential Statistics: Coefficients, p-values, and Bootstrap CIs")



```

All of the models yield the same result, showing there is a violation in the age variable but all other variables are significant.

```{r}

kable(round(cbind(btc.wd, btr.wd),4), caption="width of the two bootstrap confidence intervals")


```

We see from the above analysis that the two values are very similar to each other.


# Summary

We see from our findings above that all three of our models come to the same conclusion. We will be using our initial final model where we used a log transformation because they seem to be a little more significant. 

Again we see that the final model can be represented as log(price)=3.9245 − 0.0246×Average_Drive + 0.1866×Greens_on_reg + 
0.0387×Save_percent − 0.0181×Number_events + 0.1826×Age_Above_30TRUE

From the above data we can also conclude that as average winnings go up our average drive goes down by -2.5% while other variables such as greens on regulation and save percent go up.

## Conclusion 
The main conclusion we can draw is that while driving distance in golf is perceived to be a big indication of whether a person is winning or not we do not see this in our analysis. While the Average Winnings goes up the average drive goes down by -2.5% while Save Percentage goes up by 3.9% and Greens on Regulation goes up by 1.9% which shows that the "short" game in golf (putting and chipping) is more important than driving in regards to average money won.

We see one flaw in our analysis and it was that age was not significant which could mess up our analysis. in the future we could push our age up or down for the true or false statement to see if that changes the significance. We could also look at some values of age to see if there are any outliers, for example golfers are not younger than 20 but they can be 45+ which could mess with our significance.

We can use this model for seasons to come to see if things may have changed. For example since golf has evolved recently to players having longer drives we could run this model again in 2024 to see if the "short" game is less likely to win more money than the "long" game would.

